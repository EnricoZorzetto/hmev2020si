---
title: "Supporting Information for the manuscript: Bayesian non-asymptotic extreme value models for environmental data"
author: "Enrico Zorzetto, Antonio Canale, and Marco Marani"
date: "4/30/2020"
output: pdf_document
bibliography: hbev_bib.bib
header-includes: \usepackage{multirow}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

 \renewcommand{\thefigure}{S\arabic{figure}}
 \renewcommand{\thetable}{S\arabic{table}}
 \renewcommand{\theequation}{S\arabic{equation}}

# Details on the Bayesian GEV and POT models implemented



Here we breafly review the main extreme value statistical models consistently with the notation used in the main manuscript. The two approaches detailed below are the Generalized Extreme Value distribution used as a model for block maxima series, and the Peak Over Threshold (POT) defined as a Poisson point process. For a complete discussion, see Coles [-@coles2001introduction] or De Haan and Ferreira [-@de2007extreme]. These models are commonly used for EV analysis and statistical software that implement these technique is available, such as for example the \emph{extRemes} R package [@gilleland2016extremes]. The Bayesian estimation for the GEV and POT models used in our study is described below and implemented in the HMEV R package.
 
##  The Generalized Extreme Value distribution

The (GEV) distribution [@von1936distribution] has cdf
\begin{equation}
    Pr \left(Y \le y\right) =  F_{GEV} \left(y \mid \mu, \sigma, \xi \right) = \exp \left\{ -\left( 1 + \frac{\xi}{\sigma}\left( y - \mu \right)
    \right)_{+}^{-1/\xi} \right\}.
    \label{eq:gev}
\end{equation}
where $\mu \in R$ and $\sigma \in R^{+}$ are location and scale
parameters respectively, while $\xi \in R$ is a shape parameter, and $(\cdot)_{+} = \max\left\{ 0, \cdot \right\}$. Depending on the value of $\xi$, the GEV family encompasses a double exponential, an heavy-tailed, and an upper bounded  distribution. 

The GEV parameters are generally estimated by means of Maximum Likelihood (ML), Penalized ML [@martins2000generalized], L-Moments [@hosking1990moments] or Bayesian methods [@coles1996bayesian, @coles2003fully]. Confidence intervals can be obtained with the Delta method in the case of ML inference [@coles2001introduction] or using Bootstrap techniques when the L-moments are used to fit the distribution. Generally L-moments perform better than ML in the case of small samples, even though asyntotic theory for confidence intervals is not available. Bayesian Methods allow better characterization of the variability of extimated values [@coles1996bayesian; @coles1996bayesian2; @coles2003fully; @stephenson2004bayesian]. Here we use Bayesian methods for fitting the GEV model (we implemented a Stan model, sampling from the posterior using the Hamiltonian Monte Carlo sampler as done for HMEV). This gives us Bayesian probability intervals for the GEV quantiles for a given return time. We elicit the prior distributions for the three GEV parameters as follows: For the shape parameter, we select as prior distribution a normal distribution centered in 0.114 with a standard deviation $\sigma = 0.125$. This choice matches the expected value suggested globally for daily rainfall extremes  [@koutsoyiannis2004statistics], while the overall shape of the Prior distribution closely match the Geophysical Prior proposed by Martins and Stedinger [-@martins2000generalized] in order to guide inference towards reaslistc values of the shape parameter in our application. For the shape and scale parameters, we select informative priors centered around the mean and standard deviation of the annual maxima samples, in order to exploit prior knoiwledge on the expected value and characteristic variability of annual maxima values.


<!-- %\begin{itemize} -->
<!-- %    \item For the location parameter: $\mu \sim N(0, 100)$  -->
<!-- %    \item For the scale parameter: $\sigma \sim \Gamma(0.0001, 0.00001)$  -->
<!-- %    \item For the shape parameter: $\xi \sim N( 0.114, 0.125)$  -->
<!-- %    \end{itemize} -->

##  The Peak Over Threshold method
The GEV distribution also arises as limiting model for the block maxima of a point process with Poisson-distributed arrival of events with magnitudes distributed according to a Generalized Pareto distribution, which is often used to model exceedance over a high threshold [@davison1990models]. The distribution of exceedances over a high threshold $q$ is 

\begin{equation}
    Pr \left( Y > y\right) = Pr \left( X > x + q \lvert X > q\right) \simeq
    \frac{1 - F(x + q)}{1 - F(u)} = 1 - F_{GPD} \left( x \lvert q, \beta,
    \kappa \right)
    = \left(
    1 + \frac{\kappa}{\beta}\left( x - \mu \right)\right)^{-1/\kappa}
\end{equation}

In this case, the distribution of block maxima reads

\begin{equation}
    F_{PP} \left(x \right) = \sum_{n = o}^{\infty}p_n(n \lvert \lambda)F_{GPD} \left(x \lvert
    \beta, q, \kappa \right)
    =1 - \sum_{n = o}^{\infty} \frac{\lambda^n e^{-\lambda}}{n!}\left(
    1 + \frac{\kappa}{\beta}\left( x - q\right)\right)^{-1/\kappa}
    = F_{GEV} \left( x \lvert \mu, \sigma, \xi \right)
\end{equation}

where the parameters of the GEV are obtained as follows $\kappa = \xi$, $\beta = \sigma +\xi \left( q - \mu \right)$, and $\lambda = \left( 1 + \xi \frac{\xi}{\beta}\left( q - \mu \right)\right)^{-1/\xi}$

Here we use Bayesian inference for this Poisson-GPD model for threshold
exceedances. We elicit the prior distribution for the model parameters as
follows: For the Pareto shape parameter, we choose the same informative prior elicited for the GEV - annual maxima model. For the Poisson rate, we select a prior distribution centered on $4$ events/year, which appears reasonable value since we fix the threshold based on a fixed number of average exceedances in each block. For our analysis, we choose an automated threshold such that $5\%$ of the non-zero values are above threshold.


<!-- %\begin{itemize} -->
<!-- %    \item For the Poisson rate parameter: $\lambda \sim uniform(0, 365)$ -->
<!-- %    \item For the scale parameter: $\beta \sim \Gamma(0.0001, 0.00001)$ -->
<!-- %    \item For the shape parameter: $\kappa \sim uniform(-0.5, 0.5)$ -->
<!-- %\end{itemize} -->

Once we draw from the posterior samples  for these parameters
$\lambda^{(s)}$, $\beta^{(s)}$, and $\kappa^{(s)}$, for $s = 1, \dots, S$, we can
compute the posterior probability for the quivalent parameters of the GEV
model as follows:

   $\mu^{(s)} = q + \frac{\beta^{(s)}}{\kappa^{(s)}} \left( \lambda^{(s) ^{\kappa^{(s)}}} - 1\right)$
   $\sigma^{(s)} = \beta^{(s)} \lambda^{(s) ^{\kappa^{(s)}}}$
   $\xi^{(s)} = \kappa^{(s)}$



<!-- %The threshold $q$ is chosen according to one of the following methods: (a) making sure that the results are not overly sensitive to the choice of the threshold, (b) high enough for the asymptotic hypothesis to hold (e.g., 75 percentile or higher?), and (c), studying the Mean Excess Function (MEF), as suggested e.g., in -->
<!-- [@coles2001introduction]. -->

<!-- % figures here -->

## Definitions of lppd and lpml



Evaluating the predictive accuracy of extreme value models in estimating the right tail of the  distribution is indeed an inherently challenging task, as high quantiles are, by definition, poorly represented in the  available samples. For this reason, cross validation techniques are rarely used to assess the performance of fitted extreme value models. In our analysis, however, we harnessed the considerable length of the synthetic data sets available here in order to extensively test the performance of different methods using both in-sample and out-of-sample validation techniques. To this end, we employ both in-sample and out-of-sample measures.

The log pointwise predictive density (lppd) [@gelman2013bayesian] computed both for the in-sample data and for the out-of-sample data is often used as a measure of global performance of the models. 

This can be directly estimated from $S$ draws from a MCMC sameple as

\begin{equation}
	    \widehat{lppd} = \sum_{i = 1}^{n} \log \left( \frac{1}{S}\sum_{s = 1}^{S} p(y_i \lvert
    {\theta}^{(s)})\right)
\end{equation}
This quantity, if computed for in-sample data $y_i$, is expected to overestimate the expected log predictive density (elpd) for the same data points. This overestimation is generally corrected by quantifying the overfit of the model using some estimate of the effective number of parameter of the model. Common corrections used in practice include the Deviance Information Crtiterion (DIC), Watanaibe-Aikake information criterion (WAIC) [@gelman2013bayesian], or leave one out techniques such as the log posterior marginal likelihood (LPML) [@gelfand1994bayesian], or leave-one-out based on Pareto Smoothed Importance Sampling (PSIS) , [@vehtari2017practical]. 

Here we used the logarithm of the pseudo-marginal likelihood (lpml), a convenient index that directly accounts, at no additional computational cost, for a leave-one-out cross validation measure [@gelfand1994bayesian]. Notably, since the lpml approximates the expected log pointwise predictive density, the difference between the in-sample lppd and the lpml represents the number of effective parameters of a model [@vehtari2017practical] and thus will be used to quantify overfitting. 
 
\begin{equation}
    lpml_m = -\frac{1}{m}\sum_{i = 1}^{m} \log{\left( CPO_i \right)}
\end{equation}

where we add the factor $-1/m$ to reduce its variation with sample size.
Here $CPO_i$ is the \emph{Conditional Predictive Ordinate} statistics introduced by
[@gelfand1992model] and [@gelfand1994bayesian], which estimates the
probability of observing a value $y_i$ given that $\mathbf{y}_{-i}$ has been
observed. $CPO_i$ can be obtained as follows:

\begin{equation}
    CPO_i =  \left\{\int \frac{1}{p\left( y_i \mid
        \mathbf{\theta}\right)} p \left( \mathbf{\theta} \mid
    \mathbf{y_{-1}} \right) d \mathbf{\theta} \right\}^{-1}
\end{equation}

The CPO can be computed as the geometric mean of the likelihood of the data
(annual maxima) given the model. Sampling from the posterior, one can compute$CPO_i$ as follows: 

\begin{equation}
    \widehat{CPO_i} = \left[ \frac{1}{S} \sum_{s = 1}^{S} \frac{1}{p\left( y_i \mid
    \mathbf{\theta}^{\left(s\right)}\right)}\right]^{-1}
\end{equation}

Therefore $CPO_i$ can be computed as the harmonic mean of the likelihood of the
annual maxima $y_i$ given the model. The best model amongst the ones tested
here will be characterized by the largest value of $LPML$.
Note that given the properties of this measure of predictive performance
(in essence equivalent to a leave-one-out cross validation), we can use LPML
when a single sample is available for calibration and testing of the EV models.
Therefore, LPML can be used to decide which of these models is best for a given
sample/ application. We therefore propose to use it when only short samples are
available to decide which EV model we shouls use for the distribution of annual
maxima. We note that LPML can be difficult to use for the GEV distribution when applied to a validation sample different from the one used to fit the distribution, since it is possible that the modelled posterior distribution assign zero probability to some of the observed values.





\section{Additional tables}

\begin{table}[h]
	\centering
	\caption{Summary of 4 model specifications used to generate synthetic datasets in the simulation study.}
	\begin{tabular}{ll}

		\hline
		Model for $x_{ij}$  & parameters \\ \hline

		GP  & $\xi = 0.1, \quad \sigma = 8$ \\
		GAM  & $\alpha = 1.2, \quad \beta = 0.12$ \\
		WEI  & $\gamma = 0.6, \quad \delta = 8 $ \\
		WEI$_G$  & $\mu_{\delta} = 7, \quad \sigma_{\delta} = 1, \quad \mu_{\gamma} = 1, \quad \sigma_{\gamma} = 0.1,$ \\
		\hline
		Model for $n_{j}$  & parameters \\ \hline
		BBN & $ \mu_n = 100, \quad \sigma^2_n = 150 $  \\ \hline

		\label{tab:simulations}
	\end{tabular}
\end{table}

\begin{table}
	\centering
	\caption{Values of the constants used to elicit the prior hyperparameters of the model for event magnitudes.}
	\begin{tabular}{ llll }
		% \begin{tabular}{ lccc }
		\hline
		Parameter  & Value  \\ \hline
		\hline
		$\alpha_{\mu \gamma 0}=$   &  $i_{\mu \gamma 0} $ \\
		$\beta_{\mu \gamma 0}=$   &  $i_{\mu \gamma 0} \cdot e_{\mu \gamma 0} $ \\

		$\alpha_{\mu \delta 0}=$   &  $i_{\mu \delta 0} $ \\
		$\beta_{\mu \delta 0}=$   &  $i_{\mu \delta 0} \cdot e_{\mu \delta 0} $ \\

		$\alpha_{\sigma \gamma 0}=$   &  $i_{\sigma \gamma 0} $ \\
		$\beta_{\sigma \gamma 0}=$   &  $i_{\sigma \gamma 0} \cdot e_{\sigma \gamma 0} \cdot v_{\sigma \gamma 0} $ \\

		$\alpha_{\sigma \delta 0}=$   &  $i_{\sigma \delta 0} $ \\
		$\beta_{\sigma \delta 0}=$   &  $i_{\sigma \delta 0} \cdot e_{\sigma \delta 0} \cdot  v_{\sigma \delta 0} $ \\


		\hline
		Constant  & Value & Meaning & Empirical Prior \\ \hline

		$i_{\mu \gamma 0}$  & 10 & shape informativeness  & no \\
		$i_{\mu \delta 0}$  & 10 & scale informativeness & no \\

		$i_{\sigma \gamma 0}$  & 10 & shape informativeness & no \\
		$i_{\sigma \delta 0}$  & 10 & scale informativeness & no \\

		$e_{\mu \gamma 0}$  & 0.7 & expected value shape & no \\
		$e_{\mu \delta 0}$  & $10/ \Gamma \left( 1 + 1/{0.7}\right)$ & expected value scale &  no (possible)\\

		$v_{\sigma \gamma 0}$  & 0.05 & expected variability shape & no \\
		$v_{\sigma \delta 0}$  & 0.25 & expected variability scale &  no (possible) \\



		\hline



		\label{tab:priors}
	\end{tabular}
\end{table}


%
\begin{table}
	\centering
	\caption{Summary of the model for the number of arrivals.}
	\begin{tabular}{ lccc }
		% \begin{tabular}{ lccc }
		\hline
		Model  & Outer level & Inner Level & Prior \\ \hline
		\multirow{1}{*}{ Binomial }


		& $n_j \sim Bin(\pi_0)$ &  & $\pi_0 \sim Beta\left( 2, 2\right)$
		\\
		\hline

		\multirow{3}{*}{ Beta Binomial }  & $n_j \sim Bin(\pi_0)$ & $\pi_0 \sim Beta\left( \alpha_n, \beta_n\right)$   & $\mu_n ~ \Gamma \left( 10, 0.1 \right)$
		\\
		& & & $\omega_n ~ \Gamma \left( 0.2, 0.2 \right)$
		\\
		& &    & $ \alpha_n = \left( 1 - \frac{\left(\omega_n + 1 \right)}{ \left( N_t - \mu_n \right) } \right) / \left(   \frac{\left(\omega_n + 1 \right)}{ \left( N_t - \mu_n \right) } N_t - 1\right) $
		\\

		& &    & $ \beta_n = \frac{\alpha_n}{\mu_n} \left( N_t - \mu_n\right)$
		\\

		\hline


		\label{tab:nmodels}
	\end{tabular}
\end{table}


## Stations: Effective number of parameters

As we can see in Figure \ref{fig:effnumpar} we report the effective number of paramaters computed for observed data. Additionally, we include the same quantity computed in a different way (Watanabe-Aikake information criterion) to show that results do not change too much.

```{r effnumpar, echo=FALSE, fig.lp = "fig:", fig.cap="Effective number of parameters for the stations in the USHCN dataset.\\label{fig:effnumpar}", out.width = '100%'}
knitr::include_graphics("../output/outplot/eff_num_par_waic2_kfold_479_0_dec.png")
```



## Spatial distribution of the results for LPPD and FSE

As we can see in Figure \ref{fig:maps_lppd}, here are the results for the three models
evaluated using the LPPD

Here we provide a spatially-explicit representation of model performances, by mapping, in Figures \ref{fig:maps_lppd} and \ref{fig:maps_fse}, the best model for each station as evaluated through the lppd pr FSE measures respectively. This representation of the results of our analysis shows again the interesting difference observed for the in-sample analysis, which tend to favor the POT method, and the out-of-sample results where HMEV appears to be selected most often as preferred model. The frequency of HMEV being the model of choice is higher for smaller sample sizes. 

```{r maps1, echo=FALSE,  fig.lp = "fig:", fig.cap="Best model for each station, as evaluated through the LPPD measure (in-sample and out-of-sample).\\label{fig:maps_lppd}", out.width = '100%'}
knitr::include_graphics("../output/outplot/lppd_maps_kfold_479_0_dec.png")
```

And in Figure \ref{fig:maps_fse} we report the same result for the FSE

```{r maps2, echo=FALSE,  fig.lp = "fig:", fig.cap="Best model for each station, as evaluated through the FSE measure (in-sample and out-of-sample).\\label{fig:maps_fse}", out.width = '100%'}
knitr::include_graphics("../output/outplot/fse_maps_kfold_479_0_dec.png")
```

in Figures \ref{fig:wei_spec}, \ref{fig:gam_spec}, and \ref{fig:gpd_spec} we report examples using the Weibull, Gamma, and Generalized Pareto specifications respectively.

```{r wei, echo=FALSE,  fig.lp = "fig:", fig.cap="Example of fit to samples of 20 and 50 yearly blocks of data generated according to the Weibull specification.\\label{fig:wei_spec}", out.width = '100%'}
knitr::include_graphics("../output/outplot/synth_examples_ss_20_50_4spec_wei.png")
```

```{r gam, echo=FALSE,  fig.lp = "fig:", fig.cap="Example of fit to samples of 20 and 50 yearly blocks of data generated according to the Generalized Pareto specification.\\label{fig:gpd_spec}", out.width = '100%'}
knitr::include_graphics("../output/outplot/synth_examples_ss_20_50_4spec_gpd.png")
```

```{r gpd, echo=FALSE,  fig.lp = "fig:", fig.cap="Example of fit to samples of 20 and 50 yearly blocks of data generated according to the Gamma specification.\\label{fig:gam_spec}", out.width = '100%'}
knitr::include_graphics("../output/outplot/synth_examples_ss_20_50_4spec_gam.png")
```

<!-- ## Including R code -->

<!-- ```{r cars} -->
<!-- summary(cars) -->
<!-- ``` -->

<!-- ## Including Plots -->

<!-- You can also embed plots, for example: -->

<!-- ```{r pressure, echo=FALSE} -->
<!-- plot(pressure) -->
<!-- ``` -->

<!-- Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot. -->

# References